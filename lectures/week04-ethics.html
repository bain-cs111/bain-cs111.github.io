<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1.0"> <link rel="icon" href="/assets/images/favicon.ico" type="image/x-icon"> <meta http-equiv="X-UA-Compatible" content="IE=Edge"> <link id="github-css" rel="stylesheet" href="/assets/main.css"> </head> <title>Ethics Module 3 - Bias</title> <body class="two-column"> <div class="wrapper"> <article> <div class="main"> <object hidden="hidden" id="assignment-flag-custom"></object> <object hidden="hidden" id="canvas_id">1481043</object> <object hidden="hidden" id="due_date">2024-10-13</object> <object hidden="hidden" id="canvas_assignment_group">Ethics Modules</object> <object hidden="hidden" id="canvas_submission_types">online_upload,online_text_entry</object> <object hidden="hidden" id="canvas_allowed_extensions">pdf,docx,doc,txt,rtf</object> <object hidden="hidden" id="canvas_points_possible">2</object> <p>While AI has the potential to revolutionize work and recreation, its use can lead to unintentional bias and discrimination. One of the key issues with AI bias is that it can perpetuate existing inequalities and reinforce stereotypes. For example, biased AI algorithms used in hiring processes can lead to discrimination against certain groups based on race, gender, or other factors. Similarly, AI used in criminal justice systems can exacerbate existing biases and result in unfair treatment of certain individuals. The data the models are trained on can lead to sampling bias, confirmation bias, and selection bias, among others. If these biases are not identified and addressed, they can lead to inaccurate and unfair outcomes.</p> <p>As AI continues to be integrated into various industries, it is important to consider the potential for bias and take steps to mitigate its impact. This includes evaluating the data used to train AI algorithms and ensuring that diverse perspectives are represented. It also involves being mindful of the potential for unintended consequences and actively working to address bias in AI systems. This module introduces students to the issue of bias via an interactive game, and the discussion applies students’ observations about the game to a real-world case study of the harms of algorithmic bias.</p> <hr> <h2 id="part-1---complete-the-pre-discussion-activity">Part 1 - Complete the Pre-Discussion Activity</h2> <p>Everyone is required to complete this pre-discussion activity. Please read the article linked at the bottom of the page about <em>Racial Discrimination in Face Recognition Technology</em> written by Alex Najibi for the <em>Science in the News</em> project at Harvard University. While you are reading, consider the following questions:</p> <ul> <li>In what contexts have you interacted with facial recognition technology in the last few years?</li> <li>Do you know if your likeness is stored locally or on a server somewhere?</li> <li>Can you think of other situations where training data being selected via convenience may lead to bias in algorithmic outputs?</li> </ul> <blockquote> <p>Note: while the reading is accessible via the link below, the site that’s hosting the article has an issue with its security certificate. The site is <em>not</em> malicious but if you’re worried about security you can <a href="https://bain-cs111.github.io/course-files/ethics/module3_bias.pdf"> also download a PDF version of the article here.</a></p> </blockquote> <hr> <h2 id="part-2---attend-your-discussion-section">Part 2 - Attend your Discussion Section</h2> <p>You should have registered for a discussion section this week by joining the appropriate Canvas group. Switching sessions is not allowed (unless there is a direct academic conflict or emergency) and attendance at your registered section is required and you must be in-attendance <em>for the entirety of the session</em>. For questions about discussion sections please post on edSTEM (if it’s a general question) or email Prof. Bain (<a href="mailto:connor.bain@northwestern.edu">connor.bain@northwestern.edu</a>) if it’s specific to your case.</p> <hr> <h2 id="part-3---submit-your-self-reflection">Part 3 - Submit your Self Reflection</h2> <p>After attending your discussion section, be sure to return here to complete this self-reflection. Reflect on your discussion section (200-300 words): What strategies could be used to audit systems for bias? Was there anything you particularly liked or disliked about the discussion, or any other feedback? Make sure to submit your thoughts either via text entry or file upload in Canvas.</p> <hr> <h3 style="padding-top:15px;" id="readings">1. Assigned Readings</h3> <ul> <li> Alex Najibi for <em>Science in the News</em> at Harvard University. - <a href="https://sitn.hms.harvard.edu/flash/2020/racial-discrimination-in-face-recognition-technology/" target="_blank" rel="external nofollow noopener">Racial Discrimination in Face Recognition Technology</a>. <em>(Note: while this is a general summary, many of the linked studies dive much deeper into the issues.)</em> </li> </ul> </div> </article> </div> </body> </html>